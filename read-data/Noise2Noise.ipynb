{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Noise2Noise.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"cHITXHJvAEGU","colab_type":"code","outputId":"bf9cb1ba-c850-4f8d-bc4d-c3d2ac19d212","executionInfo":{"status":"ok","timestamp":1574713427241,"user_tz":-60,"elapsed":2546,"user":{"displayName":"Sorin-Sebastian Mircea","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAGpf54zfs5Ao0ZN9qg3HGt2zvywC6pAWmRXuro4g=s64","userId":"12935862574100330107"}},"colab":{"base_uri":"https://localhost:8080/","height":63}},"source":["import os,sys,shutil\n","import tensorflow as tf\n","import numpy as np\n","import argparse\n","import cv2,math,glob,random,time\n","import time\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import cv2\n","import glob\n","import random"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"wGo-iuvEmNNg","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"JMVkqmK9ATVK","colab_type":"code","colab":{}},"source":["# Model\n","REGULARIZER_COF = 1e-6\n","\n","def _fc_variable( weight_shape,name=\"fc\"):\n","    with tf.variable_scope(name):\n","        # check weight_shape\n","        input_channels  = int(weight_shape[0])\n","        output_channels = int(weight_shape[1])\n","        weight_shape    = (input_channels, output_channels)\n","\n","        # define variables\n","        weight = tf.get_variable(\"w\", weight_shape     , initializer=tf.contrib.layers.xavier_initializer())\n","        bias   = tf.get_variable(\"b\", [weight_shape[1]], initializer=tf.constant_initializer(0.0))\n","    return weight, bias\n","\n","def _conv_variable( weight_shape,name=\"conv\"):\n","    with tf.variable_scope(name):\n","        # check weight_shape\n","        w = int(weight_shape[0])\n","        h = int(weight_shape[1])\n","        input_channels  = int(weight_shape[2])\n","        output_channels = int(weight_shape[3])\n","        weight_shape = (w,h,input_channels, output_channels)\n","        # define variables\n","        weight = tf.get_variable(\"w\", weight_shape     , initializer=tf.contrib.layers.xavier_initializer_conv2d())\n","        bias   = tf.get_variable(\"b\", [output_channels], initializer=tf.constant_initializer(0.0))\n","    return weight, bias\n","\n","def _deconv_variable( weight_shape,name=\"conv\"):\n","    with tf.variable_scope(name):\n","        # check weight_shape\n","        w = int(weight_shape[0])\n","        h = int(weight_shape[1])\n","        output_channels = int(weight_shape[2])\n","        input_channels  = int(weight_shape[3])\n","        weight_shape = (w,h,input_channels, output_channels)\n","        regularizer = tf.contrib.layers.l2_regularizer(scale=REGULARIZER_COF)\n","        # define variables\n","        weight = tf.get_variable(\"w\", weight_shape    ,\n","                                initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n","                                regularizer=regularizer)\n","        bias   = tf.get_variable(\"b\", [input_channels], initializer=tf.constant_initializer(0.0))\n","    return weight, bias\n","\n","def _conv2d( x, W, stride):\n","    return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = \"SAME\")\n","\n","def _deconv2d( x, W, output_shape, stride=1):\n","    # x           : [nBatch, height, width, in_channels]\n","    # output_shape: [nBatch, height, width, out_channels]\n","    return tf.nn.conv2d_transpose(x, W, output_shape=output_shape, strides=[1,stride,stride,1], padding = \"SAME\",data_format=\"NHWC\")\n","\n","\n","def _conv_layer(x, input_layer, output_layer, stride, filter_size=3, name=\"conv\", isTraining=True):\n","    conv_w, conv_b = _conv_variable([filter_size,filter_size,input_layer,output_layer],name=\"conv\"+name)\n","    h = _conv2d(x,conv_w,stride=stride) + conv_b\n","    h = tf.contrib.layers.batch_norm(h, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, is_training=isTraining, scope=\"gNormc\"+name)\n","    h = tf.nn.leaky_relu(h)\n","    return h\n","\n","def _deconv_layer(x,input_layer, output_layer, stride=2, filter_size=3, name=\"deconv\", isTraining=True):\n","    bs, h, w, c = x.get_shape().as_list()\n","    deconv_w, deconv_b = _deconv_variable([filter_size,filter_size,input_layer,output_layer],name=\"deconv\"+name )\n","    h = _deconv2d(x,deconv_w, output_shape=[bs,h*2,w*2,output_layer], stride=stride) + deconv_b\n","    h = tf.contrib.layers.batch_norm(h, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, is_training=isTraining, scope=\"gNormd\"+name)\n","    h = tf.nn.leaky_relu(h)\n","    return h\n","\n","def buildGenerator(x,reuse=False,isTraining=True,nBatch=64,resBlock=4,name=\"generator\"):\n","\n","    with tf.variable_scope(name, reuse=reuse) as scope:\n","        if reuse: scope.reuse_variables()\n","\n","        h = _conv_layer(x, 3, 64, 1, 7 , \"i-1_g\")\n","\n","        tmp = h\n","\n","        for i in range(resBlock):\n","            conv_w, conv_b = _conv_variable([3,3,64,64],name=\"res%s-1\" % i)\n","            nn = _conv2d(h,conv_w,stride=1) + conv_b\n","            nn = tf.contrib.layers.batch_norm(nn, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, is_training=isTraining, scope=\"Normr%s-1_g\" %i)\n","            nn = tf.nn.leaky_relu(nn)\n","            conv_w, conv_b = _conv_variable([3,3,64,64],name=\"res%s-2\" % i)\n","            nn = _conv2d(nn,conv_w,stride=1) + conv_b\n","            nn = tf.contrib.layers.batch_norm(nn, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, is_training=isTraining, scope=\"Normr%s-2_g\" %i)\n","\n","            nn = tf.math.add(h,nn, name=\"resadd%s\" % i)\n","            h = nn\n","\n","        conv_w, conv_b = _conv_variable([3,3,64,64],name=\"conv3-1_g\" )\n","        h = _conv2d(h,conv_w,stride=1) + conv_b\n","        h = tf.contrib.layers.batch_norm(h, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, is_training=isTraining, scope=\"Norm3-1_g\")\n","        h = tf.math.add(tmp,h, name=\"add\")\n","\n","        h = _conv_layer(h, 64, 64, 1, 3 , \"3-2_g\")\n","\n","\n","        conv_w, conv_b = _conv_variable([7,7,64,3],name=\"convo_g\" )\n","        h = _conv2d(h,conv_w,stride=1) + conv_b\n","        y = tf.nn.tanh(h)\n","\n","    return y\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NTFwn75PAcU-","colab_type":"code","colab":{}},"source":["# Data Generator\n","class BatchGenerator:\n","    def __init__(self, img_size, datadir):\n","        self.folderPath = datadir\n","        self.imagePath = glob.glob(self.folderPath+\"/*\")\n","        #self.orgSize = (218,173)\n","        self.imgSize = (img_size,img_size)\n","        assert self.imgSize[0]==self.imgSize[1]\n","\n","\n","    def add_mosaic(self,img,ocp):\n","        #flg = True\n","        occupancy = ocp#np.random.uniform(min_occupancy, max_occupancy)\n","        mosaic =img\n","        h, w, _ = img.shape\n","        img_for_cnt = np.zeros((h, w), np.uint8)\n","\n","        while True:\n","            ms_size = np.random.randint(8,16)\n","            scale = np.random.randint(4,8)\n","            x = random.randint(0, max(0, w - 1 - ms_size))\n","            y = random.randint(0, h - 1 - ms_size)\n","            area = img[y:y+ms_size,x:x+ms_size]\n","            area = cv2.resize(area,(ms_size//scale,ms_size//scale), interpolation=cv2.INTER_NEAREST)\n","            area = cv2.resize(area,(ms_size,ms_size), interpolation=cv2.INTER_NEAREST)\n","            mosaic[y:y+ms_size,x:x+ms_size] = area\n","            area = np.where(area>0, 255, area)\n","            img_for_cnt[y:y+ms_size,x:x+ms_size] = cv2.cvtColor(area, cv2.COLOR_RGB2GRAY)\n","            #print((img_for_cnt > 0).sum())\n","            if (img_for_cnt > 0).sum() > h * w * occupancy:\n","\n","                break\n","        return mosaic\n","\n","    def add_impulse(self, img, ocp):\n","        rate = np.random.rand() * ocp/2 + ocp/2\n","        mask = np.random.binomial(size=img.shape, n=1, p=rate)\n","        noise = np.random.randint(256, size=img.shape)\n","        img = img * (1 - mask) + noise * mask\n","        return img\n","\n","\n","    def add_noise(self, img, ocp):\n","        #noise = (np.random.rand(self.imgSize[0],self.imgSize[1],3) -0.5) * 512\n","        rate = np.random.rand() * ocp/2 + ocp/2\n","        noise = np.random.normal(0,100,(self.imgSize[0],self.imgSize[1],3))\n","        occupy = np.random.binomial(n=1,p=rate,size=[self.imgSize[0],self.imgSize[1],3])\n","        noise = noise * occupy\n","        img = img+noise\n","        img = np.clip(img,0,255)\n","        return img\n","\n","    def strongNoise(self, img, ntype=\"gauss\"):\n","        height = img.shape[0]\n","        width = img.shape[1]\n","        #print(height,width)\n","        noise = np.zeros_like(img)\n","\n","        if ntype == \"random\":\n","            noise = (np.random.rand(height,width,3) - 0.5) * 255\n","\n","        elif ntype == \"gauss\":\n","            noise = (np.random.randn(height,width,3) - 0.5) * 255\n","\n","        elif ntype == \"sandp\":\n","            noise = (np.random.rand(height,width) - 0.5) * 255\n","            noise = np.where( noise > 255/4, 255, 0)\n","            noise2 =(np.random.rand(height,width) - 0.5) * 255\n","            noise2 = np.where( noise2 > 255/4, 0, -255)\n","            noise += noise2\n","            noise = np.tile(noise,3).reshape(height,width,3)\n","\n","        new = img + noise\n","        return new\n","\n","    def getBatch(self, nBatch, id, ocp=0.5):\n","        x   = np.zeros( (nBatch,self.imgSize[0],self.imgSize[1],3), dtype=np.float32)\n","        for i,j in enumerate(id):\n","            if j >= len(self.imagePath):\n","                continue\n","            img = cv2.imread(self.imagePath[j])\n","            dmin = min(img.shape[0],img.shape[1])\n","            img = img[int(0.5*(img.shape[0]-dmin)):int(0.5*(img.shape[0]+dmin)),int(0.5*(img.shape[1]-dmin)):int(0.5*(img.shape[1]+dmin)),:]\n","            img = cv2.resize(img,self.imgSize)\n","\n","            #img = self.add_noise(img,ocp)\n","            #img = self.add_mosaic(img,ocp)\n","            #img = self.add_impulse(img,ocp)\n","\n","\n","            x[i,:,:,:] = (img - 127.5) / 127.5 \n","\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YerrJZtKBQzb","colab_type":"code","colab":{}},"source":["def loss_g(y, t):\n","    #mse = tf.reduce_mean(tf.square(y - t))\n","    loss = tf.nn.l2_loss(y-t) + tf.reduce_sum(tf.abs(y-t))\n","    return loss\n","\n","\n","def training(loss):\n","    optimizer = tf.train.AdamOptimizer(learning_rate=0.01,\n","                                       beta1=0.9,\n","                                       beta2=0.999)\n","    train_step = optimizer.minimize(loss)\n","    return train_step\n","\n","def tileImage(imgs):\n","    d = int(math.sqrt(imgs.shape[0]-1))+1\n","    h = imgs[0].shape[0]\n","    w = imgs[0].shape[1]\n","    r = np.zeros((h*d,w*d,3),dtype=np.float32)\n","    for idx,img in enumerate(imgs):\n","        idx_y = int(idx/d)\n","        idx_x = idx-idx_y*d\n","        r[idx_y*h:(idx_y+1)*h,idx_x*w:(idx_x+1)*w,:] = img\n","    return r\n","\n","def foloderLength(folder):\n","    dir = folder\n","    paths = os.listdir(dir)\n","    return len(paths)\n","\n","def printParam(scope):\n","    total_parameters = 0\n","    for variable in tf.trainable_variables(scope=scope):\n","        # shape is an array of tf.Dimension\n","        shape = variable.get_shape()\n","        variable_parameters = 1\n","        for dim in shape:\n","            variable_parameters *= dim.value\n","        total_parameters += variable_parameters\n","    print(\"{} has {} parameters\".format(scope, total_parameters))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iLgIRHo0CFYA","colab_type":"code","outputId":"eca30d3c-4866-4aff-fe81-bb9d98c4ebcc","executionInfo":{"status":"ok","timestamp":1574713427245,"user_tz":-60,"elapsed":2517,"user":{"displayName":"Sorin-Sebastian Mircea","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAGpf54zfs5Ao0ZN9qg3HGt2zvywC6pAWmRXuro4g=s64","userId":"12935862574100330107"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EeGCP0bzrA2e","colab_type":"code","outputId":"357a7201-440d-46c8-e3fd-9ab1e9c13e7e","executionInfo":{"status":"ok","timestamp":1574713429897,"user_tz":-60,"elapsed":5161,"user":{"displayName":"Sorin-Sebastian Mircea","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAGpf54zfs5Ao0ZN9qg3HGt2zvywC6pAWmRXuro4g=s64","userId":"12935862574100330107"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["!ls \"/drive/My Drive/ML_Project\"\n","ROOT_PATH = \"/drive/My Drive/ML_Project/\"\n","DATASET_DIR = ROOT_PATH + \"data\"\n","VAL_DIR = ROOT_PATH + \"val\"\n","SAVE_DIR = ROOT_PATH + \"model\"\n","SVIM_DIR = ROOT_PATH + \"samples\""],"execution_count":0,"outputs":[{"output_type":"stream","text":[" data\t\t\t     model-train-from-scratch   samples\n","'initial training results'   Noise2Noise.ipynb\t        val\n"," model\t\t\t    'pre-trained results'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pMnxcJuGsMuv","colab_type":"code","colab":{}},"source":["def main():\n","    if not os.path.exists(SAVE_DIR):\n","        os.makedirs(SAVE_DIR)\n","    if not os.path.exists(SVIM_DIR):\n","        os.makedirs(SVIM_DIR)\n","\n","    img_size = 256\n","    bs = 16\n","\n","    dir = DATASET_DIR\n","    val = VAL_DIR\n","    datalen = foloderLength(DATASET_DIR)\n","    vallen = foloderLength(VAL_DIR)\n","\n","    # loading images on training\n","    batch = BatchGenerator(img_size=img_size,datadir=dir)\n","    val = BatchGenerator(img_size=img_size,datadir=val)\n","\n","    id = np.random.choice(range(datalen),bs)\n","    IN_ = tileImage(batch.getBatch(bs,id)[:4])\n","\n","    IN_ = (IN_ + 1)*127.5\n","    cv2.imwrite(\"input.png\",IN_)\n","\n","\n","    start = time.time()\n","\n","    x = tf.placeholder(tf.float32, [bs, img_size, img_size, 3])\n","    t = tf.placeholder(tf.float32, [bs, img_size, img_size, 3])\n","\n","    y =buildGenerator(x,nBatch=bs)\n","\n","    loss = loss_g(y, t)\n","    printParam(scope=\"generator\")\n","\n","    train_step = training(loss)\n","\n","    init = tf.global_variables_initializer()\n","    sess = tf.Session()\n","    sess.run(init)\n","    saver = tf.train.Saver()\n","    summary = tf.summary.merge_all()\n","\n","    ckpt = tf.train.get_checkpoint_state(SAVE_DIR)\n","\n","    if ckpt: # checkpointがある場合\n","        last_model = ckpt.model_checkpoint_path # 最後に保存したmodelへのパス\n","        print (\"load \" + last_model)\n","        saver.restore(sess, last_model) # 変数データの読み込み\n","        print(\"succeed restore model\")\n","    else:\n","        print(\"models were not found\")\n","        init = tf.global_variables_initializer()\n","        sess.run(init)\n","\n","    print(\"%.4e sec took initializing\"%(time.time()-start))\n","\n","\n","    hist =[]\n","\n","\n","    start = time.time()\n","    for i in range(100000):\n","        # loading images on training\n","        id = np.random.choice(range(datalen),bs)\n","        batch_images_x = batch.getBatch(bs,id,ocp=0.5)\n","        batch_images_t = batch.getBatch(bs,id,ocp=0.5)\n","\n","        tmp, yloss = sess.run([train_step,loss], feed_dict={\n","            x: batch_images_x,\n","            t: batch_images_t\n","        })\n","\n","        print(\"in step %s loss = %.4e\" %(i,yloss))\n","        hist.append(yloss)\n","\n","        if i %40 ==0:\n","            id = np.random.choice(range(vallen),bs)\n","            batch_images_x = val.getBatch(bs,id,ocp=0.5)\n","            out = sess.run(y,feed_dict={\n","                x:batch_images_x})\n","            X_ = tileImage(batch_images_x[:4])\n","            Y_ = tileImage(out[:4])\n","\n","            X_ = (X_ + 1)*127.5\n","            Y_ = (Y_ + 1)*127.5\n","            Z_ = np.concatenate((X_,Y_), axis=1)\n","            #print(np.max(X_))\n","            cv2.imwrite(\"{}/{}.png\".format(SVIM_DIR,i),Z_)\n","\n","            fig = plt.figure()\n","            ax = fig.add_subplot(111)\n","            plt.title(\"Loss\")\n","            plt.grid(which=\"both\")\n","            plt.yscale(\"log\")\n","            ax.plot(hist,label=\"test\", linewidth = 0.5)\n","            plt.savefig(\"hist.png\")\n","            plt.close()\n","\n","            print(\"%.4e sec took per 100steps\" %(time.time()-start))\n","            start = time.time()\n","\n","        if i%40==0 :\n","            if i>1900:\n","                loss_1k_old = np.mean(hist[-2000:-1000])\n","                loss_1k_new = np.mean(hist[-1000:])\n","                print(\"old loss=%.4e , new loss=%.4e\"%(loss_1k_old,loss_1k_new))\n","                if loss_1k_old*2 < loss_1k_new:\n","                    break\n","\n","            saver.save(sess,os.path.join(SAVE_DIR,\"model.ckpt\"),i)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GeSKXtm9sOfV","colab_type":"code","outputId":"c2446bcd-a8f7-43fc-a546-4058d55403b9","executionInfo":{"status":"error","timestamp":1574713652356,"user_tz":-60,"elapsed":2192,"user":{"displayName":"Sorin-Sebastian Mircea","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAGpf54zfs5Ao0ZN9qg3HGt2zvywC6pAWmRXuro4g=s64","userId":"12935862574100330107"}},"colab":{"base_uri":"https://localhost:8080/","height":514}},"source":["main()"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-64e5dfe2ffa2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mbuildGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnBatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_g\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-cabed3f57ac7>\u001b[0m in \u001b[0;36mbuildGenerator\u001b[0;34m(x, reuse, isTraining, nBatch, resBlock, name)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_conv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"i-1_g\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-cabed3f57ac7>\u001b[0m in \u001b[0;36m_conv_layer\u001b[0;34m(x, input_layer, output_layer, stride, filter_size, name, isTraining)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_conv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misTraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mconv_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_conv_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilter_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilter_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conv\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_conv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconv_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconv_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates_collections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0misTraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gNormc\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-cabed3f57ac7>\u001b[0m in \u001b[0;36m_conv_variable\u001b[0;34m(weight_shape, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mweight_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# define variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_shape\u001b[0m     \u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_initializer_conv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mbias\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput_channels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1241\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m   def _get_partitioned_variable(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    565\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m   def _get_partitioned_variable(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    517\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     synchronization, aggregation, trainable = (\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 868\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    869\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Variable generator/convi-1_g/w already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n"]}]}]}